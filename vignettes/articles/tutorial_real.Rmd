---
title: "2d Temporal Model: Tutorial on real data"
author: "Francesco Serafini"
date: "2023-06-27"
output: html_document
---

```{r setup0, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "png",
  dev.args = list(type = "cairo-png"),
  echo = TRUE
)
```

```{r setup, include=TRUE,message=FALSE,warning=FALSE}
library(ETAS.inlabru)
library(ggplot2)

# Increase num.cores if you have more cores on your computer, and reduce it
# if you have fewer!
num.cores <- 2

INLA::inla.setOption(num.threads = num.cores)
```


# Introduction to ETAS model

In this tutorial, we show how to use the `ETAS.inlabru` R-package to fit a temporal ETAS model on real earthquakes data. The tutorial shows how to prepare the data, how to fit a model, how to retrieve the posterior distribution of the parameters and the posterior distribution of other quantities of interest, how to generate synthetic catalogues from the fitted model, and how to produce forecasts of seismicity. 

The Epidemic-Type Aftershock Sequence (ETAS) model belongs to the family of Hawkes (or \emph{self-exciting}) point processes. The temporal Hawkes process is a point process model with conditional intensity given by

$$
\lambda(t | \mathcal H_t) = \mu + \sum_{t_h \in \mathcal H_t} g(t- t_h)
$$
where $\mathcal H_t$ is the history of the process up to time $t$. Generally speaking $\mathcal H_t$ contains all the events occurred \emph{strictly} before $t$. The quantity $\mu > 0$ is usually called the \emph{background rate}, and is interpreted as the rate at which events occur spontaneously. The function $g(t-t_h)$ is called \emph{triggering} function (or excitation function, or simply kernel) and measures the influence of having an event in $t_h$ on time $t$. If we look at $g(t-t_h)$ as a function of $t$ is the intensity of the point process representing the offspring of the event in $t_h$. In seismology the offspring of an event are called \emph{aftershocks}, the two terms will be used as synonyms. In essence, an Hawkes process model can be seen as the superposition of a background process with intensity $\mu$ and all the aftershock processes generated by the observations in $\mathcal H_t$ each one with intensity $g(t-t_h)$. This makes Hawkes process model particularly suitable to describe phenomena in which an event has the ability to trigger additional events, phenomena
characterized by cascades of events such as earthquakes, infectious diseases, wildfires, financial crisis, and similar.

The ETAS model is a particular instance of Hawkes process model which has proven to be particularly suitable to model earthquake occurrence. Earthquakes are usually described and modelled as marked time points where the marking variable is the magnitude of the event. So the history of the process is composed by time-magnitude pairs, namely $\mathcal H_t = \{(t_h,m_h), h = 1,...,N_h\}$. Various slightly different ETAS formulations exist, usually characterized by slightly different triggering functions, the one that is implemented in the `ETAS.inlabru` R-package has conditional intensity given by

$$
\lambda(t | \mathcal H_t) = \mu + \sum_{t_h \in \mathcal H_t} K \exp\{\alpha(m_h - M_0)\}\left(\frac{t - t_h}{c} + 1\right)^{-p}
$$
where $M_0$ is the cutoff magnitude such that $m_h \geq M_0$ for any $h$. This value is decided a priori based on the quality of the catalogue used. 

The parameters of the model are: 

* $\mu \geq 0$, the background rate
* $K \geq 0$ a general productivity parameter, it plays a role in determining the average number of aftershocks induced by any event in the catalogue. 
* $\alpha \geq 0$ a magnitude scaling parameter, it determines how the number of aftershocks changes based on the magnitude of the event generating the aftershocks. It has to be non-negative to reflect the fact that stronger earthquakes generate more aftershocks.
* $c > 0$ a time offset parameter, smaller values are associated with catalogues with fewer missing events. 
* $p > 1$ an aftershock decay parameter, it determines the rate at which the aftershock activity decreases over time. It has to be greater than 1 otherwise an event may generate an infinite number of aftershocks in an infinite interval of time which is thought to be unphysical.

# Priors

As any other Bayesian analysis we need to decide the priors of the parameters. The approximation method that we use considers the parameters in two different scales: the original ETAS scale, and the internal scale. The internal scale is used by the package to perform the calculations. In the internal scale the parameters does not have any constraint and have a standard normal distribution as prior. We need to set up the priors for the parameters in the ETAS scale. This is done by considering a copula transformation $\eta(X)$ such that if $X \sim N(0,1)$ then, $\eta(X)$ has the desired distribution. 

The `ETAS.inlabru` R-package provides four different functions corresponding to three different distributions:

1. `gamma_t(X, a, b)` for a Gamma distribution with shape parameter $a$ and rate $b$. The distribution is such that the mean is $a/b$ and the variance is $a/b^2$.

2. `unit_t(X, a, b)` for a Uniform distribution between $a$ and $b$.

3. `exp_t(X, r)` for an Exponential distribution with rate $r$.

4. `loggaus_t(X, m ,s)` for a Log-Gaussian distribution with mean of the logarithm $m$ and standard deviation of the logarithm $s$.

The code below generate 1000 observations from a normal distribution, transform them using the functions provided by the package, and shows the empirical density estimated from the sample. 

```{r}
# obtain sample from standard normal distribution
X <- rnorm(1000)
# apply copula transformations
gamma.X <- gamma_t(X, 1, 2)
unif.X <- unif_t(X, 0, 1)
exp.X <- exp_t(X, 1)
loggaus.X <- loggaus_t(X, 0.5, 0.5)

# build data.frame for plotting
df.to.plot <- rbind(
  data.frame(
    value = X,
    distribution = "Std Normal"
  ),
  data.frame(
    value = gamma.X,
    distribution = "Gamma"
  ),
  data.frame(
    value = unif.X,
    distribution = "Uniform"
  ),
  data.frame(
    value = exp.X,
    distribution = "Exponential"
  ),
  data.frame(
    value = loggaus.X,
    distribution = "Log-Gaussian"
  )
)
# plot them
ggplot(df.to.plot, aes(value)) +
  geom_histogram() +
  theme_bw() +
  facet_wrap(facets = ~distribution, scales = "free")
```

The package also provide inverse functions to retrieve the value of a parameter in the internal scale if the value in the ETAS scale is provided. Below an example

```{r}
inv_gamma_t(gamma_t(1.2, 1, 2), 1, 2)
inv_unif_t(unif_t(1.2, 1, 2), 1, 2)
inv_exp_t(exp_t(1.2, 1), 1)
inv_loggaus_t(loggaus_t(1.2, 1, 2), 1, 2)
```

Once we have decided the priors that we are going to use in the analysis, we need to store the corresponding copula transformations in a `list`. The `list` one element for each parameter of the model ($5$), and each element of the list must have name corresponding to the parameter. The names are fixed and should be `mu`, `K`, `alpha`, `c_`, and `p`. The parameter $c$ is referred as `c_` to avoid clashing names with the R function `c()`. It is useful to do the same with the inverse functions also, this list will be used to set the initial value of the parameters later. The code below assumes that parameter $\mu$ has a Gamma distribution as prior with parameters $0.3$ and $0.6$, parameters $K, \alpha,$ and $c$ have a Uniform prior on $(0,10)$, while the parameter $p$ has a Uniform prior on $(1,10)$.

```{r}
# set copula transformations list
link.f <- list(
  mu = \(x) gamma_t(x, 0.3, 0.6),
  K = \(x) unif_t(x, 0, 10),
  alpha = \(x) unif_t(x, 0, 10),
  c_ = \(x) unif_t(x, 0, 10),
  p = \(x) unif_t(x, 1, 10)
)

# set inverse copula transformations list
inv.link.f <- list(
  mu = \(x) inv_gamma_t(x, 0.3, 0.6),
  K = \(x) inv_unif_t(x, 0, 10),
  alpha = \(x) inv_unif_t(x, 0, 10),
  c_ = \(x) inv_unif_t(x, 0, 10),
  p = \(x) inv_unif_t(x, 1, 10)
)
```

# L'Aquila seismic sequence

Earthquake data is stored in the so-called earthquake catalogues. Many different
catalogues exists for the same region and there is no easy way to decide which
one is better. Here, we provide a subset of the HOmogenized instRUmental Seismic
(HORUS) catalogue from 1960 to 2020. It can be downloaded from [http://horus.bo.ingv.it/](http://horus.bo.ingv.it/).
See [data-raw/horus.R](https://github.com/edinburgh-seismicity-hub/ETAS.inlabru/blob/main/R/AnalysingPosteriors.R) for details on how the the subset was created from the original data set.
This HORUS catalogue subset can be accessed directly using `horus` or `ETAS.inlabru::horus`:

```{r}
# load HORUS catalogue
head(horus)
```
Further documentation of the data set is available via `?horus`.

The data reports for each earthquake the time as a string (`time_string`), longitude (`lon`) and latitude (`lat`) of the epicentre, the depth in kilometres (`depth`), and the moment magnitude (`M`).

To focus on the L'Aquila seismic sequence is sufficient to retain only the observations in a specific space-time-magnitude region that include the sequence of interest. For the L'Aquila sequence, we retain all the events with magnitude greater or equal than $2.5$ happened during 2009 with longitude in $(10.5, 16)$ and latitude in $(40.5, 45)$. The L'Aquila sequence selected in this way should be composed by 1024 events. Any other seismic sequence can be selected similarly.

To do the selection is convenient to transform the time string in a `Date` object and to select the rows of the Horus catalogue verifying the conditions.

```{r}
# transform time string in Date object
horus$time_date <- as.POSIXct(
  horus$time_string,
  format = "%Y-%m-%dT%H:%M:%OS"
)
# There may be some incorrectly registered data-times in the original data set,
# that as.POSIXct() can't convert, depending on the system.
# These should ideally be corrected, but for now, we just remove the rows that
# couldn't be converted.
horus <- na.omit(horus)

# set up parameters for selection
start.date <- as.POSIXct("2009-01-01T00:00:00", format = "%Y-%m-%dT%H:%M:%OS")
end.date <- as.POSIXct("2010-01-01T00:00:00", format = "%Y-%m-%dT%H:%M:%OS")
min.longitude <- 10.5
max.longitude <- 16
min.latitude <- 40.5
max.latitude <- 45
M0 <- 2.5

# set up conditions for selection
aquila.sel <- (horus$time_date >= start.date) &
  (horus$time_date < end.date) &
  (horus$lon >= min.longitude) &
  (horus$lon <= max.longitude) &
  (horus$lat >= min.latitude) &
  (horus$lat <= max.latitude) &
  (horus$M >= M0)

# select
aquila <- horus[aquila.sel, ]
```

The data can be visually represented by plotting the time of each event against the magnitude. This shows the clustering of the observations in correspondance of high magnitude events.

```{r, fig.cap = "L'Aquila seismic sequence, times versus magnitudes"}
ggplot(aquila, aes(time_date, M)) +
  geom_point() +
  theme_bw()
```

# Data preparation to model fitting

We need to prepare a `data.frame` to be used as input data to fit the ETAS model. This `data.frame` must have three columns: `ts` for the time difference between the starting date and the occurrence time of the events (measured in days in this example), `magnitudes` for the magnitude of the events, and `idx.p` which is an index column with a different value for each event. The names are fixed and should not be changed 

```{r}
# set up data.frame for model fitting
aquila.bru <- data.frame(
  ts = as.numeric(
    difftime(aquila$time_date, start.date, units = "days")
  ),
  magnitudes = aquila$M,
  idx.p = 1:nrow(aquila)
)
```

After that, we need to set up the initial values of the parameters and the `list` containing the `inlabru` options to be used. The initial values should be stored in a `list` with elements `th.mu`, `th.K`, `th.alpha`, `th.c`, and `th.p` which corresponds to the ETAS parameters. The initial values must be provided in the internal scale and therefore it is useful to retrieve them using the inverse copula transformations that we set up before. In this way, we can find the values of the parameters in the internal scale given the value of the parameters in the ETAS scale. The example below uses $\mu = 0.5, K = 0.1, \alpha = 1, c = 0.1,$ and $p = 1.1$ as initial values. It is crucial to set initial values that do not cause numerical problems, in general this is achieved by setting initial values that are not \emph{too close} nor \emph{too far} from zero. The values provided below have worked well in various examples.  

```{r}
# set up list of initial values
th.init <- list(
  th.mu = inv.link.f$mu(0.5),
  th.K = inv.link.f$K(0.1),
  th.alpha = inv.link.f$alpha(1),
  th.c = inv.link.f$c_(0.1),
  th.p = inv.link.f$p(1.1)
)
```

Lastly, we need to set the `list` of `inlabru` options. The main elements of the `list` are :

* `bru_verbose`: number indicating the type of diagnostic output. Set it to 0 for no output.
* `bru_max_iter`: maximum number of iterations. If we do not set `max_step` the `inlabru` algorithm stops when the stopping criterion is met. However, setting `max_step` to values smaller than 1 forces the algorithm to run for exactly `bru_max_iter` iterations.
* `bru_method`: for what is relevant here, the only thing that we may need to set is the `max_step` argument. If the algorithm does not converge without fixing a `max_step` then we suggest to try to fix it to some value below 1, in our experience $0.5$ or $0.2$ works well. In the example below the line setting `bru_method` is commented.
* `bru_initial`: `list` of initial values created before.

```{r}
# set up list of bru options
bru.opt.list <- list(
  bru_verbose = 3, # type of visual output
  bru_max_iter = 70, # maximum number of iterations
  # bru_method = list(max_step = 0.5),
  bru_initial = th.init # parameters' initial values
)
```

Note: In the option list, one can also set the number of threads allowed for
INLA here, e.g. with `num.threads = 8`.  This would override the global option
that was set at the beginning of this tutorial.  For code likely to be run on
many different systems, using the global setting is easier to manage.


# Model fitting

The function `Temporal.ETAS` fit the ETAS model and returns a `bru` object as output. The required inputs are: 

* `total.data`: `data.frame` containing the observed events. It have to be in the format described in the previous Section.
* `M0`: cutoff magnitude. All the events in `total.data` must have magnitude greater or equal to this number.
* `T1`: starting time of the time interval on which we want to fit the model.
* `T2`: end time of the time interval on which we want to fit the model.
* `link.functions`: `list` of copula transformation functions in the format described in previous sections.
* `coef.t.`, `delta.t.`, `N.max.`: parameters of the temporal binning. The binning strategy is described in Appendix B of the paper  [Approximation of Bayesian Hawkes process with `inlabru`](https://onlinelibrary.wiley.com/doi/full/10.1002/env.2798). The parameters corresponds to `coef.t.`$=\delta$, `delta.t.`$=\Delta$, and `N.max.`$=n_{\max}$.
* `bru.opt`: `list` of `inlabru` options as described in the previous Section.

```{r, message=FALSE}
# set starting and time of the time interval used for model fitting. In this case, we use the interval covered by the data.
T1 <- 0
T2 <- max(aquila.bru$ts) + 0.2 # Use max(..., na.rm = TRUE) if there may still be NAs here
# fit the model
aquila.fit <- Temporal.ETAS(
  total.data = aquila.bru,
  M0 = M0,
  T1 = T1,
  T2 = T2,
  link.functions = link.f,
  coef.t. = 1,
  delta.t. = 0.1,
  N.max. = 5,
  bru.opt = bru.opt.list
)
```


# Create input list 

After a model has been fitted the package `ETAS.inlabru` offers various functions to visually explore the output. They all require in input a `list`. The `list` must have different elements depending on the function we are going to use. To retrieve the posterior of the parameters and to sample from the posterior of the parameters we only need two elements: 

* `model.fit`: the output of `Temporal.ETAS`
* `link.functions`: the list of copula transformations

```{r}
# create input list to explore model output
input_list <- list(
  model.fit = aquila.fit,
  link.functions = link.f
)
```

# Check marginal posterior distributions

The function `get_posterior_param` can be use to retrieve the marginal posteriors of the parameters in the ETAS scale. The function returns a `list` with elements:

* `post.df`: a `data.frame` containing the posterior of the parameters. The `data.frame` has three columns, `x` with the value of the parameter, `y` with the corresponding value of the posterior, and `param` that indicates which ETAS parameter `x` and `y` are referring to.

* `post.plot`: a `ggplot` object containing the plot of the marginal posteriors of the parameters


```{r}
# get marginal posterior information
post.list <- get_posterior_param(input.list = input_list)

# plot marginal posteriors
post.list$post.plot
```

# Sample the joint posterior and make pair plot

The function `post_sampling` generate samples from the joint posterior of ETAS parameters. The function takes in input:

* `input.list`: a list with a `model.fit` element and a `link.functions` elements as described above.
* `n.samp`: number of posterior samples.
* `max.batch`: the number of posterior samples to be generated simultaneously. If `n.samp`$>$`max.batch`, then, the samples are generated in parallel in batches of maximum size equal to `max.batch`. Default is $1000$.
* `ncore`: number of cores to be used in parallel when `n.samp`$>$`max.batch`.

The function returns a `data.frame` with columns corresponding to the ETAS parameters

```{r}
post.samp <- post_sampling(
  input.list = input_list,
  n.samp = 1000,
  max.batch = 1000,
  ncore = num.cores
)

head(post.samp)
```

The posterior samples can be used to analyse the correlation between parameters. The function `post_pairs_plot` generate a pair plot of the posterior samples taken as input. The function has 4 arguments but they do not need to be all specified. The input are: 

* `post.samp`: `data.frame` of samples from the joint posterior distribution of the parameters. If it is `NULL` than the samples are generated by the function itself.
* `input.list`: an input list with arguments `model.fit` and `link.functions` to be used to generate the posterior samples. This is used only if `post.samp = NULL`. Default is `NULL`.
* `n.samp`: number of posterior samples. If it is `NULL`, then the samples in `post.samp` are used. If `post.samp` is `NULL`, then `n.samp` samples are generated from the joint posterior. If both `post.samp` and `n.samp` are not `NULL` then `n.samp` samples are randomly (uniformly and with replacement) selected from `post.samp`. Default is `NULL`
* `max.batch` the number of posterior samples to be generated simultaneously. Same as before and only used when `post.samp` is NULL. Default is `NULL`

The function returns a `list` with two elements: `post.samp` with the posterior samples, and `pair.plot` which is a `ggplot` object containing the pair plot.

```{r}
pair.plot <- post_pairs_plot(
  post.samp = post.samp,
  input.list = NULL,
  n.samp = NULL,
  max.batch = 1000
)
pair.plot$pair.plot
```


# Check posterior number of events

A quantity of interest is the posterior distribution of the number of events. This can be accessed using the function `get_posterior_N` which requires as only input a `list`. However, the `list` needs to have some additional elements with respect the one used until now. Specifically, we need to add `T12` with the extremes of the time interval of which we want to calculate the number of events, `M0` the cutoff magnitude, and `catalog.bru` a `data.frame` containing the observed events. The latter has to be in the same format as `total.data` used for the `Temporal.ETAS` function.

The function returns a list of three elements: `post.plot` with a plot of the distribution, `post.plot.shaded` with a plot of the distribution with shaded regions representing $95\%$ interval for the distribution, and `post.df` with the `data.frame` used to generate the plots. The vertical line in the plots represent the number of events in the `catalog.bru` element of the input list.

```{r}
# set additional elements of the list
input_list$T12 <- c(T1, T2)
input_list$M0 <- M0
input_list$catalog.bru <- aquila.bru
```

```{r}
N.post <- get_posterior_N(input.list = input_list)
N.post$post.plot
```

# Posterior of the triggering function and Omori law

The functions `triggering_fun_plot` and `triggering_fun_plot_prior` plot, respectively, the quantiles of the posterior and prior distribution of the triggering function $g(t-t_h, mh)$, namely,

$$
g(t - t_h, m_h) = K\exp\{\alpha(m_h - M_0)\}\left(\frac{t - t_h}{c} + 1\right)^{-p}
$$
The function takes in input 

* `input.list`: the input list as defined for the functions used previously.
* `post.samp`: a `data.frame` of samples from the posterior distribution of the parameters. If it is `NULL`, then `n.samp` samples are generated from the posterior.
* `n.samp`: number of posterior samples of the parameters to be used or generated.
* `magnitude`: the magnitude of the event ($m_h$).
* `t.end`: the maximum value of $t$ for the plot.
* `n.breaks`: the number of breaks in which the interval $(0, \texttt{t.end})$ is divided.

The function returns a `ggplot` object. For each sample of the parameters the triggering function between $0$ and `t.end` is calculated. The black solid lines represents the $95\%$ posterior interval of the function, the grey lines represent the triggering function calculated with the posterior samples, and the horizontal red lines represent the $95\%$ posterior interval of the background rate $\mu$. 

The function `triggering_fun_plot_prior` does the same but the value of the parameters are sampled according to the prior distribution rather than the posterior, and therefore, we do not need to specify a posterior samples `data.frame`.

```{r}
triggering_fun_plot(
  input.list = input_list,
  post.samp = post.samp,
  n.samp = NULL, magnitude = 4,
  t.end = 5, n.breaks = 100
)
```

```{r}
triggering_fun_plot_prior(input.list = input_list, magnitude = 4, n.samp = 1000, t.end = 10)
```

The functions `omori_plot_posterior` and `omori_plot_prior` do the same as the functions `triggering_fun_plot` and `triggering_fun_plot_prior` but considering only 

$$
\left(\frac{t- t_h}{c} + 1\right)^{-p}
$$
instead of the whole triggering function and without the background rate.

```{r}
omori_plot_posterior(input.list = input_list, post.samp = post.samp, n.samp = NULL, t.end = 5)
```

```{r}
omori_plot_prior(input.list = input_list, n.samp = 1000, t.end = 5)
```

# Generate synthetic catalogues from model

An earthquake forecast is usually composed by a collection of synthetic catalogues from a model. The package `ETAS.inlabru` provides a function to generate synthetic catalogues for a given set of parameters. This can be used both to produce forecasts or to simply produce synthetic catalogues. The function to generate synthetic catalogues is called `generate_temporal_ETAS_synthetic` which takes as input 

* `theta`: a `list` of ETAS parameters with names `mu`, `K`, `alpha`, `c`, and `p`, corresponding to the ETAS parameters.
* `beta.p`: the parameter of the magnitude distribution
* `M0`: cutoff magnitude, all the generated event will have magnitude greater than `M0`.
* `T1`: starting time of the catalogue (the unit of measure depends on the unit used to fit the model).
* `T2`: end time of the catalogue (the unit of measure depends on the unit used to fit the model).
* `Ht`: set of known events. They can also be between `T1` and `T2`, this is useful when we want to generate catalogues with imposed events.  

Regarding the magnitude distribution, it is an exponential, specificically we assume 

$$
m - M_0 \sim \text{Exp}(\beta)
$$
The parameter $\beta$ is usually estimated independently from the ETAS parameters. We use the maximum likelihood estimator for $\beta$ which is given by

$$
\hat\beta = \frac{1}{\bar{m} - M_0}
$$
where $\bar m$ is the mean of the observed magnitudes values.

```{r}
# maximum likelihood estimator for beta
beta.p <- 1 / (mean(aquila.bru$magnitudes) - M0)
```

The function returns a `list` of `data.frame`, each element of the output `list` corresponds to a different generation. The `data.frame` have three columns: occurence time (`ts`), magnitude (`magnitudes`), a the generation identifier (`gen`). The generation identifier uses the following convention, $-1$ indicates the events in `Ht` with time between `T1` and `T2`, $0$ indicates the first generation offspring of the events with `gen` equal $-1$, $1$ indicates background events, $2$ all the offspring of the events with `gen` equal $0$ or $1$, $3$ indicates all the offspring of the events with `gen` equal $2$, $4$ indicates all the offspring of the events with `gen` equal $3$, and so on. To obtain a unique `data.frame` containing all the simulated events it is sufficient to bind by rows all the generations.

Below an example in which we generate 1 synthetic catalogue using as parameters one of the posterior samples generated before. The catalogue covers the same time span as the L'Aquila catalogue and we impose the greatest event in the sequence. 

```{r}
synth.cat.list <- generate_temporal_ETAS_synthetic(
  theta = post.samp[1, ], # ETAS parameters
  beta.p = beta.p, # magnitude distribution parameter
  M0 = M0, # cutoff magnitude
  T1 = T1, # starting time
  T2 = T2, # end time
  Ht = aquila.bru[which.max(aquila.bru$magnitudes), ] # known events
)
# merge into unique data.frame
synth.cat.df <- do.call(rbind, synth.cat.list)
# order events by time
synth.cat.df <- synth.cat.df[order(synth.cat.df$ts), ]

ggplot(synth.cat.df, aes(ts, magnitudes, color = as.factor(gen))) +
  geom_point(size = 0.5)
```

We can easily generate multiple catalogues. The code below generates 8 catalogues using different samples from the posterior distribution of the parameters. The red point indicates the event that we have imposed and the last panel represents the observed L'Aquila sequence. 

```{r}
set.seed(2)
n.cat <- 8
# generate catalogues as list of lists
multi.synth.cat.list <- lapply(seq_len(n.cat), \(x)
generate_temporal_ETAS_synthetic(
  theta = post.samp[x, ],
  beta.p = beta.p,
  M0 = M0,
  T1 = T1,
  T2 = T2,
  Ht = aquila.bru[which.max(aquila.bru$magnitudes), ]
))

# store catalogues as list of data.frames
multi.synth.cat.list.df <- lapply(multi.synth.cat.list, \(x) do.call(rbind, x))
# set catalogue identifier
multi.synth.cat.list.df <- lapply(seq_len(n.cat), \(x) cbind(multi.synth.cat.list.df[[x]],
  cat.idx = x
))
# merge catalogues in unique data.frame
multi.synth.cat.df <- do.call(rbind, multi.synth.cat.list.df)

# we need to bing the synthetics with the observed catalogue for plotting
cat.df.for.plotting <- rbind(
  multi.synth.cat.df,
  cbind(aquila.bru[, c("ts", "magnitudes")],
    gen = NA,
    cat.idx = "observed"
  )
)

# plot them
ggplot(cat.df.for.plotting, aes(ts, magnitudes)) +
  geom_point(size = 0.5) +
  geom_point(
    data = aquila.bru[which.max(aquila.bru$magnitudes), ],
    mapping = aes(ts, magnitudes),
    color = "red"
  ) +
  facet_wrap(facets = ~cat.idx)
```

# Forecasting

An earthquake forecast is usually collection of synthetic catalogues generated from a model. For bayesian models we can reflect the uncertainty on the parameters values by generating each synthetic catalogue composing the forecast using a different set of parameters sampled from the join posterior distribution. We can generate forecasts using the function `Temporal.ETAS.forecast`. The function takes as input 

* `post.samp`: a `data.frame` of samples from the posterior distribution of the parameters with the same format described in previous sections.
* `n.cat`: the number of synthetic catalogues composing the forecast. If `n.cat` is greater than `nrow(post.samp)`, then the rows of `post.samp` are sampled uniformly and with replacement `n.cat` times. If `n.cat` is smaller than `nrow(post.samp)`, then, then the rows of `post.samp` are sampled uniformly and without replacement `n.cat` times. If `n.cat` is `NULL` or equal to `nrow(post.samp)`, then, `post.samp` is used as it is.
* `ncore`: the number of cores to be used to generate the synthetic catalogues in parallel.

The remaining inputs (`beta.p`, `M0`, `T1`, `T2`, `Ht`) are the same as the ones used in  `generate_temporal_ETAS_synthetic`. 

The output of the function is a `list` with two elements: `fore.df` and `n.cat`. The element `fore.df` is a `data.frame` with the synthetic catalogues binded together by row, it is the same as `multi.synth.cat.df` created before. The element `n.cat` is just the number of catalogues generated. We need `n.cat` because zero-events catalogues do not appear in `fore.df`, and the corresponding `cat.idx` value is missing. Therefore we need `n.cat` to recover the total number of catalogues.

The code below creates a daily forecast for the 24 hours starting 1 minute after the event with the greatest magnitude in the sequence. The starting date and end date of the forecast are expressed in the same unit used in the catalogue to fit the model (days in this case). Day zero correspond to `start.date` stated in the beginning of the document which for the example is $2009-01-01 00:00:00$. The forecast is generated assuming known all the events in the catalogue occurred before the forecasting period.

```{r}
# express 1 minute in days
min.in.days <- 1 / (24 * 60)
# find time of the event with the greatest magnitude
t.max.mag <- aquila.bru$ts[which.max(aquila.bru$magnitudes)]
# set starting time of the forecasting period
T1.fore <- t.max.mag + min.in.days
# set forecast length
fore.length <- 1
# set end time of the forecasting period
T2.fore <- T1.fore + fore.length
# set known data
Ht.fore <- aquila.bru[aquila.bru$ts < T1.fore, ]

# produce forecast
daily.fore <- Temporal.ETAS.forecast(
  post.samp = post.samp, # ETAS parameters posterior samples
  n.cat = nrow(post.samp), # number of synthetic catalogues
  beta.p = beta.p, # magnitude distribution parameter
  M0 = M0, # cutoff magnitude
  T1 = T1.fore, # forecast starting time
  T2 = T2.fore, # forecast end time
  Ht = Ht.fore, # known events
  ncore = num.cores
) # number of cores
```

We can easily retrieve the predictive distribution of the number of events in the forecasting period looking at the frequencies with which the catalogue identifiers appears in the `fore.df` element. Indeed, the number of rows of `fore.df` with the same `cat.idx` value represents the number of events in that synthetic catalogue. So, if the frequency with which each catalogue identifier appears in `fore.df$cat.idx` correspond to the number of events in that catalogue. This allows to easily retrieve the predictive distribution of the number of events using the code below. We remark that in this case we can not use the function `table` to find the frequencies of elements of `fore.df$cat.idx`. This is because the catalogue identifier of zero-events catalogues are not present in `fore.df$cat.idx`. So using `table` would lead to have zero probability of having zero events in a day, quantity that is crucial if we are intersted in the probability of earthquake activity (probability of having at least one event).

```{r}
# find number of events per catalogue
N.fore <- vapply(
  seq_len(daily.fore$n.cat),
  \(x) sum(daily.fore$fore.df$cat.idx == x), 0
)
# find number of observed events in the forecasting period
N.obs <- sum(aquila.bru$ts >= T1.fore & aquila.bru$ts <= T2.fore)
# plot the distribution
ggplot() +
  geom_histogram(aes(x = N.fore, y = after_stat(density)), binwidth = 1) +
  geom_vline(xintercept = N.obs) +
  xlim(100, 500)
```




